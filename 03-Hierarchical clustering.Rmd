
---
title: "03-Hierarchical clustering"
output:
  html_document:
    toc: true
---


```{r}
%md
# Introduction to hierarchical clustering

```


```{r}
%md
Typically used when the number of clusters in not known a head of time
Two approaches. bottum up and top down. we will focus on bottum up process
- assign each point to its own cluster
- joing the two closesest custers/points into a new cluster
- keep going until there is one cluster
- they way you calculate the distance between clusters is a paramater and will be covered later
we have to first calculate the euclidean distance between all points (makes a big matriz) using the dist() function
- this is passed into the hclust() function
```


```{r}
data(iris)
```


```{r}
d <- dist(iris[, 1:4])
hcl <- hclust(d)
hcl
```


```{r}
?dist
```


```{r}
?hclust

```


```{r}
plot(hcl)
```


```{r}
%md
 ## Defining clusters
After producing the hierarchical clustering result, we need to cut the tree (dendrogram) at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 1.5, with would produce 2 clusters.
```


```{r}
%md
In R we can us the cutree function to

- cut the tree at a specific height: cutree(hcl, h = 1.5)
- cut the tree to get a certain number of clusters: cutree(hcl, k = 2)

Example

- Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting h.
- Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting directly k, and verify that both provide the same results.
```


```{r}
plot(hcl)
abline(h = 3.9, col = "red")
```


```{r}
cutree(hcl, k = 3)

```


```{r}
cutree(hcl, h = 3.9)
```


```{r}
identical(cutree(hcl, k = 3), cutree(hcl, h = 3.9))

```


```{r}
%md

### Challenge

Using the same value k = 3, verify if k-means and hierarchical clustering produce the same results on the iris data.

Which one, if any, is correct?
```


```{r}
km <- kmeans(iris[, 1:4], centers = 3, nstart = 10)
hcl <- hclust(dist(iris[, 1:4]))
table(km$cluster, cutree(hcl, k = 3))
```


```{r}
par(mfrow = c(1, 2))
plot(iris$Petal.Length, iris$Sepal.Length, col = km$cluster, main = "k-means")
plot(iris$Petal.Length, iris$Sepal.Length, col = cutree(hcl, k = 3), main = "Hierarchical clustering")
```


```{r}
## Checking with the labels provided with the iris data
table(iris$Species, km$cluster)
```


```{r}
table(iris$Species, cutree(hcl, k = 3))

```


```{r}
%md
### Pre-processing
Many of the machine learning methods that are regularly used are sensitive to difference scales. This applies to unsupervised methods as well as supervised methods, as we will see in the notebook.

A typical way to pre-process the data prior to learning is to scale the data, or apply principal component analysis (next notebook). Scaling assures that all data columns have a mean of 0 and standard deviation of 1.

In R, scaling is done with the scale function.

Challenge

Using the mtcars data as an example, verify that the variables are of different scales, then scale the data. To observe the effect different scales, compare the hierarchical clusters obtained on the original and scaled data.


```


```{r}
##
colMeans(mtcars)
##        mpg        cyl       disp         hp       drat         wt       qsec 
##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 
##         vs         am       gear       carb 
##   0.437500   0.406250   3.687500   2.812500

hcl1 <- hclust(dist(mtcars))
hcl2 <- hclust(dist(scale(mtcars)))
par(mfrow = c(1, 2))
plot(hcl1, main = "original data")
plot(hcl2, main = "scaled data")
```


```{r}
%md
### Additional info

Clustering linkage and practical matters
4 methods to measure distance between clusters
complete: pairwise similarty between all observations in cluster 1 and 2, uses largest of similarities
single: same as above but uses the smallest of similarities
average: same as above but uses average of similarities
centroid: finds centroid of cluster 1 and 2, uses similarity between tow centroids
rule of thumb
complete and average produce more balanced treess and are more commonly used
single fuses observations in one at a time and produces more unblanced trees
centroid can create inversion where clusters are put below single values. its not used often
practical matters
data needs to be scaled so that features have the same mean and standard deviation
normalized features have a mean of zero and a sd of one
– Linkage methods
# Cluster using complete linkage: hclust.complete
hclust.complete <- hclust(dist(x), method = "complete")

# Cluster using average linkage: hclust.average
hclust.average <- hclust(dist(x), method = "average")

# Cluster using single linkage: hclust.single
hclust.single <- hclust(dist(x), method = "single")

# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")
```


```{r}
%md
Similar to k-means , we measure the (dis)similarity of observations using distance measures (e.g., Euclidean distance, Manhattan distance, etc.); the Euclidean distance is most commonly the default. However, a fundamental question in hierarchical clustering is: How do we measure the dissimilarity between two clusters of observations? A number of different cluster agglomeration methods (i.e., linkage methods) have been developed to answer this question. The most common methods are:

- Maximum or complete linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.
- Minimum or single linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.
- Mean or average linkage clustering: Computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters. Can vary in the compactness of the clusters it creates.
- Centroid linkage clustering: Computes the dissimilarity between the centroid for cluster 1 (a mean vector of length  
p
 , one element for each variable) and the centroid for cluster 2.
- Ward’s minimum variance method: Minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance are merged. Tends to produce more compact clusters.
```

