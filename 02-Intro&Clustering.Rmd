
---
title: "02-Intro&Clustering"
output:
  html_document:
    toc: true
---


```{r}
%md
#Introduction & k-means clustering
```


```{r}
%md

In unsupervised learning, no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between

- Clustering, where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations.

- Dimensionality reduction, where the goal is to identify patterns in the features of the data. Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning.

Unsupservied learning presents specific challenges and benefits:

- there is no single goal in Unsupervised learning
- there is generally much more unlabelled data available than labelled data.

```


```{r}
%md
## k-means clustering

```


```{r}
%md
The k-means clustering algorithms aims at partitioning n observations into a fixed number of k clusters. The algorithm will find homogeneous clusters.

In R, we use

`stats::kmeans(x, centers = 3, nstart = 10)`

```


```{r}
%md

### Let's get started
```


```{r}
##prepare packages

#install.packages(“tidyverse”)       # for data work & visualization
#install.packages(“cluster”)         # for cluster modeling 
#install.packages("reshape2")        # for melting data
# note : not required if already installed
library(tidyverse)
library(cluster)
library(reshape2)
```


```{r}
%md

## Load our Dataset
```


```{r}
#load the iris dataset 
data(iris)

```


```{r}
%md
## Explore our Data Set (EDA)
```


```{r}
head(iris)
```


```{r}
glimpse(iris)
#head(iris)
#View(iris)
```


```{r}
#Visualize the data 
## Sepal-Length vs Sepal-Wdith

ggplot(iris)+
 geom_point(aes(x = Sepal.Length, y = Sepal.Width), stroke = 2)+
 facet_wrap(~ Species)+ 
 labs(x = 'Sepal Length', y = 'Sepal Width')#+
 #theme_bw()
```


```{r}
#Petal-Length vs. Petal-Width
ggplot(iris)+
 geom_point(aes(x = Petal.Length, y = Petal.Width), stroke = 2)+
 facet_wrap(~ Species)+ 
 labs(x = 'Petal Length', y = 'Petal Width')
```


```{r}
#Sepal-Length vs. Petal-Length
ggplot(iris)+
 geom_point(aes(x = Sepal.Length, y = Petal.Length), stroke = 2)+
 facet_wrap(~ Species)+ 
 labs(x = 'Sepal Length', y = 'Petal Length') #+theme_bw()
```


```{r}
#Sepal-Width vs. Pedal-Width
ggplot(iris)+
 geom_point(aes(x = Sepal.Width, y = Petal.Width), stroke = 2)+
 facet_wrap(~ Species)+ 
 labs(x = 'Sepal Width', y = 'Pedal Width')+
 theme_bw()
```


```{r}
#Box plots
ggplot(iris)+
 geom_boxplot(aes(x = Species, y = Sepal.Length, fill = Species))

```


```{r}
ggplot(iris)+
 geom_boxplot(aes(x = Species, y = Sepal.Width, fill = Species))

```


```{r}
ggplot(iris)+
 geom_boxplot(aes(x = Species, y = Petal.Length, fill = Species))

```


```{r}
ggplot(iris)+
 geom_boxplot(aes(x = Species, y = Petal.Width, fill = Species))
```


```{r}
%md
k-means clustering algorithms aims at partioning n observations into a fixed number of k clusters. 
Finding the homogeneous clusters. 

We use 
stats::kmeans(x, centers = 3, nstart = 10)

- x numeric data matrix
- centers is pre-defined # of clusters
- k-means has a random component which can be repeated nstart times to improve the returned model
```


```{r}
?kmeans

```


```{r}
%md
### Example

- To learn about k-means, let’s use the iris dataset with the sepal and petal length variables only (to facilitate visualisation). Create such a data matrix and name it x
- Run the k-means algorithm on the newly generated data x, save the results in a new variable cl, and explore its output when printed.
- The actual results of the algorithms, i.e. the cluster membership can be accessed in the clusters element of the clustering result output. Use it to colour the inferred clusters to generate a figure like that shown below.
```


```{r}
i <- grep("Length", names(iris))
x <- iris[, i]
cl <- kmeans(x, 3, nstart = 10)
plot(x, col = cl$cluster)
```


```{r}
stats::kmeans(x, centers = 3, nstart = 10)
```


### How does k-means work
- Initialisation: randomly assign class membership

```{r}
set.seed(12)
init <- sample(3, nrow(x), replace = TRUE)
plot(x, col = init)
```


```{r}
%md
k-means random intialisation

Iteration:

- Calculate the centre of each subgroup as the average position of all observations is that subgroup.
- Each observation is then assigned to the group of its nearest centre.     
It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance.
```


```{r}
par(mfrow = c(1, 2))
plot(x, col = init)
centres <- sapply(1:3, function(i) colMeans(x[init == i, ], ))
centres <- t(centres)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)

tmp <- dist(rbind(centres, x))
tmp <- as.matrix(tmp)[, 1:3]

ki <- apply(tmp, 1, which.min)
ki <- ki[-(1:3)]

plot(x, col = ki)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)
```


```{r}
%md

Termination: Repeat iteration until no point changes its cluster membership.



![Ex](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif?psid=1&width=320&height=320) 



k-means convergence (credit Wikipedia)


```


```{r}
%md
## Model Selection

```


```{r}
%md

Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest total within cluster sum of squares (SS), is selected. The total within SS is calculated as:

For each cluster results:

- for each observation, determine the squared euclidean distance from observation to centre of cluster
- sum all distances
Note that this is a local minimum; there is no guarantee to obtain a global minimum.

Challenge:

- Repeat k-means on our x data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. Try the same with random data of identical dimensions.

```


```{r}
cl1 <- kmeans(x, centers = 3, nstart = 10)
cl2 <- kmeans(x, centers = 3, nstart = 10)
table(cl1$cluster, cl2$cluster)
```


```{r}
cl1 <- kmeans(x, centers = 3, nstart = 1)
cl2 <- kmeans(x, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)
```


```{r}
set.seed(42)
xr <- matrix(rnorm(prod(dim(x))), ncol = ncol(x))
cl1 <- kmeans(xr, centers = 3, nstart = 1)
cl2 <- kmeans(xr, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)
```


```{r}
diffres <- cl1$cluster != cl2$cluster
par(mfrow = c(1, 2))
plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1))
plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1))


#Different k-means results on the same (random) data
```


```{r}
%md
### How to determine the number of clusters
- Run k-means with k=1, k=2, …, k=n
- Record total within SS for each value of k.
- Choose k at the elbow position, as illustrated below.
```


```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(x, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")
```


```{r}
%md
## In other words
### k-means Clustering
k-means clustering is a method of vector quantization, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.
Find the optimal number of clusters by Elbow Method
```


```{r}
ks <- 1:5
tot_within_ss <- sapply(ks, function(k) {
    cl <- kmeans(x, k, nstart = 10)
    cl$tot.withinss
})
plot(ks, tot_within_ss, type = "b")
```



#### How kmeans() works and practical matters
##### Process of k-means:

- randomly assign all points to a cluster
- calculate center of each cluster
- convert points to cluster of nearest center
- if no points changed, done, otherwise repeat
- calculate new center based new points
- convert points to cluster of nearest center
- and so on
##### model selection:

- best outcome is based on total within cluster sum of squares
- run many times to get global optimum
- R will automaitcally take the run with the lowest total withinss
determining number of clusters

##### scree plot
- look for the elbow
- find where addition on new cluster does not change best withinss much
- there ususally is no clear elbow in real world data

```{r}
##the elbow point : k(centers) = 3
##Apply kmeans function to the feature columns
set.seed(123)
km <- kmeans( x = iris[, -5] , centers = 3)
yclus <- km$cluster
table(yclus)
```


```{r}
#the kmeans has grouped the data into three clusters- 1, 2 & 3 having 50, 62 & 38 observations respectively.
#Visualize the kmeans clusters

clusplot(iris[, -5],
 yclus,
 lines = 0,
 shade = TRUE,
 color = TRUE,
 labels = 0,
 plotchar = FALSE,
 span = TRUE,
 main = paste('Clusters of Iris Flowers')
)
```


```{r}
#Compare the clusters
iris$cluster.kmean <- yclus
cm <- table(iris$Species, iris$cluster.kmean)
cm
```


```{r}
(50 + 48 + 36)/150 *100

#of the k-means cluster output matched with the actual Species clusters. versicolor(Cluster 2) & virginica(Cluster 3) have some overlapping features which is also apparent from the cluster visualizations.
```


```{r}

```

